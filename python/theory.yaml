-------------------------------------------------------------------------------------------------
### Data Structures ###
DS - different ways of organizing data on your computer to be used effectively.

DS Types:
Primitive:
- basic data types that cannot be broken into simpler data types.
- have a fixed size and they are usually smaller in size than the non primitive data structures.
- simpler, used for simple operations
- represented in memory as simple values
- integers: whole numbers without decimal points
- float: numbers with the decimal points
- strings: sequence of the characters or symbols enclosed in quotation marks
- booleans: values that represent true or false.

Non-primitive:
- more complex and can be broken down into smaller data types.
- can be larger in size and can grow or shrink dynamically
- more complex and can be composed of multiple primitive data structures
- used for complex operations such as data manipulation, sorting or searching
- represented in memory as pointers to other memory locations
- derived from primitive data types by combining two or more primitive data structures. 
These data structures can be subdivided as linear and non-linear data structures.

Linear:
- are those in which the elements are arranged in a sequential order with each element connected to its adjacent elements. 
These data structures are used to represent a sequence of data where the order of elements is important. 
There are many linear data structures in Python: lists, tuples, arrays, linked lists, stacks and queue.

Non-linear:
- are those in which the elements are not arranged in a sequential order. These DS are used to represent 
a hierarchical relationship between data elements where each element is connected to one or more other 
elements in a specific way. Sets, dictionaries, trees and graphs.

Built-in:
- come with Python. We don't need to use any external library or create these DSs ourselves.They are built in in Python.
- linear DS: lists and tuples are built in DS
- non linear DS: sets and dictionaries are built in DS

User Defined:
- we need to use some external library or we can create them ourselves to be able to use them.

-------------------------------------------------------------------------------------------------

### Algorithms ###
algorithm - a set of instruction to complete a task.

Types of Algorithms
Sorting: 
- used to store data in ascending or descending order
- bubble sort, selection sort, insertion sort and so on.

Searching: 
- used to find a specific value in a data set.
- linear search, binary search and some other searching algorithms.

Graph: 
- to work with data that can be represented as graph.
- depth first search, breadth first search and Dijkstra's algorithms (the most famous graph algorithms).

Dynamic programming: 
- used to solve problems by breaking them down into the smaller subproblems.
- knapsack problems

Divide and conquer: 
- used to solve problems by breaking them down in smaller subproblems,
solving each sub problem independently and combining the results.
- merge-sort or quicksort

Recursion: 
- used to solve the problems by breaking them down in smaller sub-problems that are in a similar nature.

-------------------------------------------------------------------------------------------------

Big O 
- the language and metric we use to describe the efficiency of algorithms.
- shows how the runtime of the function increases as the size of the input increases.
- we are measuring the number of operations and space complexity.

Space Complexity: 
- the amount of the memory that some code used.

Big O Notations
- Best Case: Omega
- Average Case: Big Theta
- Worst Case: Big O

Runtime Complexity
Linear Time Complexity
- time complexity will grow in direct proportion to the size of input data.

Drop Constants
- it is very possible that O(n) code is faster than O(1) code for specific input, 
but big O just describes the rate of increase.
- different computers with different architectures have different constant factors.
However, we are just interested in the algorithm, not the hardware when doing the 
asymptotic analysis, so we ignore such constant factors.

O(n^2) - nested loops:
2 loops --> n*n=n^2
3 loops --> n*n*n=n^3
In terms of Big O it doesn't matter if it is ^3, ^4, or ^10 we are still going to write it as O(n^2).
This is very inefficient code because as the number of elements increase the number of operations increase in quadratic manner.


Non Dominant Terms
Two loops:
one loop is O(n^2) - completes 100 operations
the following one is O(n) - completes 10 operation
Total time complexity is O(n^2 + n) ---> In terms of Big O we can simplify this by removing non-dominant terms (O(n^2)- dominant term, 
term with higher power, O(n) - non-dominant term, term with lower power) ---> O(n^2)


O(log n)
Divide and Conquer - searching for a number in a sorted array.
For axample and array of 8 sorted elements, we divide it 3 time until we find the required number. ---> 2^3=8 (Since we are dividing 
by 2 we are using base two notation 2^3)
2^3=8 ---> log2_8 = 3
This complexity is crucial when we work with really big numbers ---> log2_1,048,576=20
It is very efficient compared to O(n) or O(n^2) - if we increase the number of elements, the number of operations increases slightly.

Space Complexity 
- is a measure of amount of the working storage that an algorithm needs.
How much memory in the worst case is needed at any point in the algorithm.
Eg: a function summing up number from input until zero calling itself recursively.
Recursive methods like this count in the space stack. So every time each call adds a level to the stack and this takes up actual memory. 
This function takes O(n) space complexity.

Different Terms for Input - Add vs Multiply
Eg: two loops in a sequence passing n, each loop is O(n) time complexity. ---> O(n) + O(n) = O(2n) we can drop constant ---> O(n) 
If we change parameters and have one loop passing a and another one passing b instead of n ---> O(a) + O(b) = O(a+b) we cannot simplify 
it anymore. - If your algorithm is in the form "do this, then when you are all done , do that" ---> add runtimes
If we now have nested loops. Loop b inside loop a --> O(a*b) - If your algorithmm is in the form "do this for each time you do that" --->
---> multiply runtimes

How to measure the codes using Big O?
Rule 1 Any assignments statements and if statements that are executed once regardless of the size of the problem O(1)
Rule 2 A simple "for" loop from 0 to n (with no internal loops) O(n)
Rule 3 A nested loop of the same type takes quadratic time complexity O(n^2)
Rule 4 A loop, in which the controlling parameter is divided by two at each step O(log n)
Rule 5 When dealing with multiple statements, just add them up

-------------------------------------------------------------------------------------------------

Arrays - not native DS in Python (list is native to Python)
- is a DS which can store collection of elements of the same type
- store elements as contigious blocks of memory without pointers, reducing memory overhead.
- can only store elements of the same data type
- each element has a unique index

Types of Arrays
One Dimensional - linear array
Multi Dimensional
2D --> i[row index][column index]
3D --> i[depth index][row index][column index]

Types of Arrays in Memory
1D - compiler allocates 9 contiguous cells in memory, all cells next to each other. It can start in any location.
2D - in memory it is represented as one dimensional array
3D - in memory it is represented as one dimensional array

Common Operations that can be performed on Arrays

Array Module 
- more memory efficient than list for storing the large types of the same data type.
- supports only basic data types
- homogeneous (only same data types)
- import array
my_array = array.array('i')  --> i - denotes type of element integers

Numpy Module
- advantage: provides a feature rich and high performance array object, supports wide range of numerical operations and functions
- disadvantage: not part of Python standard library, you have to install additional library to be able to use it.
- import numpy
np_array = np.array([], dtype=int)

Time Complexity
- creating empty array ---> involves minimal operations such as initializing array metadata and allocating minimal 
amoount of memory for the array elements --> Time Complexity: O(1), Empty array has no elements, memory used for 
array metadata is constant and does not depend on the number of elements ---> Space Complexity: O(1)
- creating array with elements ---> involves copying elements from the input iteratively ---> Time Complexity: O(1), 
the memory allocation for the array elements depends on the number and the data type of the elements. As the number 
of elements increases, the memory needed to store those elements also increases proportionally. ---> Space Complexity: O(1)

Insertion to Array
my_array.insert(index, value) - this will insert needed value and shift indexes for other elements if needed.
Time complexity: depends on the number of elements that need to be shifted ---> O(N)
Space Complexity: when we are inserting into the array we need only one space for that element ---> O(1)

Array Traversal 
- visiting all cells of the array. Eg: printing elements one by one, or updating given element.
Time Complexity: we use for loop to traverse array --> O(n)
Space Complexity: we don't need extra location to perform traverse operation --> O(1)

Access Array Element 
- print value of cell number
arrayName[index] = "value"

def accessElement(array, index): 
    if index >= len(array): ---------------------------------------> O(1)
          print('There is no element with such index') ------------> O(1)
    else:
          print(array[index]) -------------------------------------> O(1)

Time Complexity:  because we just access element at certain index --> O(1)
Space Complexity: we do not need extra space for this operation --> O(n)

Searching for and Element in Array
Linear Search 
- iterate through elements of the array one by one, comparing each element with the target value. If the target value is found, the search is successful and you can go ahead and return the index of the target value. If the target value is not found after iterating the all elements, the search is unsuccessful and you can return an indication that the value was not found.

def linearSearch(arr, target):
    for i in range(len(arr)): --------------------------------------> O(n)
        if arr[i] == target: ---------------------------------------> O(1)
            return i  ----------------------------------------------> O(1)
        return -1     ----------------------------------------------> O(1)

Time Complexity:  total time complexity --> O(1)
Space Complexity: we do not need extra space for this operation --> O(1)

Deleting an Element from Array
- deletion is very efficient when you remove the last element, otherwise it becomes time consuming.
Time Complexity: o(n)
- if remove just last element - O(1)
- if removing any other element we need to shift following elements one step left - O(n) - worst case
Space Complexity: O(1)
- we are not taking any extra space when deleting

Time and Space Complexity of Array

Operation                       | Time Complexity | Space Complexity
---------------------------------------------------------------------
Creating empty array            |  O(1)           |   O(1)
Creating an array with elements |  O(n)           |   O(n)
Inserting value in array        |  O(n)           |   O(1)
Traversing a given array        |  O(n)           |   O(1)
Accessing a given array         |  O(1)           |   O(1)
Searching a given value         |  O(n)           |   O(1)
Deleting a given value          |  O(n)           |   O(1)


-------------------------------------------------------------------------------------------------

Two Dimensional Arrays
- an array with a bunch of values having been declared with double index - arr[i][j]
- use 2D array when you have to deal with matrix

When we create an array, we:
- assign it to a variable
- define the types of elements that it will store
- define its size (the max numbers of elements) <--- might not be needed in some languages

Creating Two Dimensional Array:
import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)

Time complexity of creating a 2D array - O(m*n) ---> m - number of columns, n - number of rows
Space complexity of creating a 2D array - O(m*n) ---> m - number of columns, n - number of rows. 
So we need this much of space in the memory.

Insertion - Two Dimensional Array
Addition of Columns:
- adding a column at the beginning of 2D array - column will be inserted in index 0, other columns shift one step right
- time complexity: O(m*n)

Addition of Rows:
- adding a row at the beginning of a 2D array - row will will inserted in index 0, other rows will be shifted one step down
- time complexity: O(m*n)



import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)
 
#Inserting row or column
newTwoDArray = np.insert(twoDArray, 0, [[1,2,3,4]], axis=1)
print(newTwoDArray)
 
#Appending row or column 
#Index is not needed as it always inserts at the end of array
newTwoDArray1 = np.insert(twoDArray,[[1,2,3,4]], axis=0)
print(newTwoDArray1)

- 0 - index of where you want to insert
- axis - to denote if its column (axis=1) or row (axis=0)
Time complexity: O(m*n)


"Access an Element of Two Dimensional Array"

array[i][j] - i is row, j is column

import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)
 
def accessElement(array, rowIndex, colIndex):
    if rowIndex >= len(array) and colIndex >= len(array[0]): #---> O(1)
        print('Incorrect index')  #------------------------------> O(1)
    else:
        print(array[rowIndex][colIndex]).  #---------------------> O(1)
 
accessElement(twoDArray,1,2)

Time complexity: O(1)
Space complexity: O(1)



"Traversal Two Dimensional Array"

import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)

def traverseTDArray(array):
    for i in range(len(array)):  # ------------------> O(mn) for each row we have to traverse all the columns
        for j in range(len(array[0])): #-------------> O(n)
            print(array[i][j])  #--------------------> O(1)
 
traverseTDArray(twoDArray)

Time complexity: O(m*n) --> if m==n --> O(n^2)
Space complexity: O(1)


"Searching Two Dimensional Array"

import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)
 
def serachTDArray(array, value):
    for i in range(len(array)):  # ------------------> O(mn) for each row we have to traverse all the columns
        for j in range(len(array[0])): #-------------> O(n)
            if array[i][j] == value:   #-------------> O(1)
                return 'The value is located at index '+str(i)+""+str(j)
    return 'The elemenet is not found'
 
print(serachTDArray(twoDArray, 14))

Time complexity: O(m*n) --> if m==n --> O(n^2)
Space complexity: O(1)


"Deletion in 2D array"
In numpy we are not straightway deleting column/row from array. We are copying the original data without deleted column into a new array.

import numpy as np
 
twoDArray = np.array([[11,15,10,16],[10,14,11,5],[12,17,12,8],[15,18,14,9]])
print(twoDArray)
 
#This deletes the first row(axis=0) 
newTDArray = np.delete(twoDArray, 0, axis = 0)
print(newTDArray)

Time complexity: O(m*n)
Space complexity: O(m*n)



Time and Space Complexity of 2D Array

Operation                       | Time Complexity | Space Complexity
---------------------------------------------------------------------
Creating empty array            |  O(1)           |   O(1)
Creating an array with elements |  O(mn)          |   O(mn)
Inserting value in array        |  O(mn)          |   O(mn)
Traversing a given array        |  O(mn)          |   O(1)
Accessing a given array         |  O(1)           |   O(1)
Searching a given value         |  O(mn)          |   O(1)
Deleting a given value          |  O(mn)          |   O(mn)


When to use/avoid Arrays

When to use:
- to store multiple variable of the same datatype
- random access

When to avoid:
- different data type elements --> if you want to declare variables of different data types, 
so for each data type we need to create separate arrays.
- array reserves memory --> Maybe it reserves a memory that will not be used in the future, 
but it takes a memory in our RAM. This also causes another problem when you add elements 
to an array and array begins to exceed its reserves capacity. In this case, the array allocates
a larger region of memory and copies its elements into the new storage.


"Python List"
- a DS that holds ordered collection of items
- the place of elements does not change
- elements don't have to be of the same type

integers = [1,2,3,4]
print(integers)
 
stringList = ['Milk', 'Cheese', 'Butter']
print(stringList)
 
mixedList = [1, 1.2, 'spam']
print(mixedList)
 
nestedList = [1,2,3,4,[1.5,1.6],[test]]
print(nestedList)
 
emptyList = []
print(emptyList)



"Accessing/Traversing the List"
#Accessing element
shoppingList = ['Milk', 'Cheese', 'Butter']
print(shoppingList[0])
 
#Accessing element, starts from the back of the list with -1
print(shoppingList[-2])
 
#Checks if element exists in list
print('Milk' in shoppingList)
 
#Traverse list
for i in shoppingList:
    print(i)
 
#Traverse and update the list
for i in range(len(shoppingList)):
    shoppingList[i] = shoppingList[i]+"+"
    print(i)


"Update/Insert into the List"

"Updating List"

myList = [1,2,3,4,5,6,7]
print(myList)
 
#Update element
myList[2] = 33
myList[3] = 55
print(myList)

Lists are ordered collections --> elements will be printed in the same order we declare them.
Time complexity: O(1)
Space complexity: O(1)


"Inserting an Element into the List"

myList = [1,2,3,4,5,6,7]
print(myList)
#Insert method, each element shifts to the right after insertion
myList.insert(0,11) #-----------------------------> O(n)
print(myList)
 
#append method, adds element at the end of list
myList.append(11)  #------------------------------> O(1)
print(myList)
 
#extend method, adds another list to existing list
newList = [11,12,13,14]
myList.extend(newList)  ##------------------------> O(n)
print(myList)



"Slice/Delete Element from List"

#Slice   
myList = ['a','b','c','d','e','f']
#prints elements index 0, until :2 ---> 2(-1)=1
print(myList[0:2])
#if you omit first index it prints elements until index :2
print(myList[:2])
#if you omit 2nd index it prints elements starting from 1st index :1 -->1(-1)=0
print(myList[1:])
#omitting both indexes prints all elements of the list
print(myList[:])
#updating using slice operator
myList[0:2] = ['x','y']
print(myList)



"Delete Element from List"

#pop method, returns popped element
#after element is popped, other element shift left one step
myList = ['a','b','c','d','e','f']
#deletes element index 1
print(myList.pop(1)) #----------------------------> O(n) - since other elements shift
#deletes last element
myList.pop() #------------------------------------> O(1) - if deleting the last element
print(myList)
 
#delete method, this function does not return anything
del myList[3] #-----------------------------------> O(n) - since other elements shift
print(myList)
 
del myList[2:4]
print(myList)
 
#remove method, removes element itself not index based
myList.remove('e')  #--------------------------- -> O(n) - since other elements shift
print(myList)


"Searching for an Element in a List"

#Searching for an element in thelist
myList = [10,20,30,40,50,60,70,80,90]
#in operator
target = 50
if in in myList: #--------------------------------> o(n) - goes through every element
    print(f"{target} is in the list")
else:
    print(f"{target} is NOT in the list")
 
#Linear Search
def linear_search(p_list, p_target):
    #enumerate function iterates through the list keeping track of the current index
    for i, value in enumerate(p_list):  #---------> O(n)
        if value == p_target:  #------------------> O(1)
            return i  #---------------------------> O(1)
        return -1  #------------------------------> O(1)
 
print(linear_search(myList,target))

Time complexity: O(n)
Space complexity: O(1)